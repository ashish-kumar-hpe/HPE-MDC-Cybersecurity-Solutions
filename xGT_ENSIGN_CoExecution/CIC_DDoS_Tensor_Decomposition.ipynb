{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import sys,os,os.path\n",
    "import linecache\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get('https_proxy'):\n",
    " del os.environ['https_proxy']\n",
    "if os.environ.get('http_proxy'):\n",
    " del os.environ['http_proxy']\n",
    "\n",
    "os.environ['ENSIGN_BASE']='/root/ashish/reservoir_lab/ENSIGN-42'\n",
    "os.environ['ANACONDA_HOME']='/root/anaconda3/'\n",
    "os.environ['PYTHONPATH']='/root/ashish/reservoir_lab/ENSIGN-42/ENSIGN_4.2/Ensign-Py3'\n",
    "\n",
    "sys.path.append('/root/ashish/reservoir_lab/ENSIGN-42/ENSIGN_4.2/bin')\n",
    "sys.path.append('/root/ashish/reservoir_lab/ENSIGN-42/ENSIGN_4.2/Ensign-Py3/bin')\n",
    "sys.path.append('/root/anaconda3/bin/')\n",
    "\n",
    "#os.environ['LD_LIBRARY_PATH']='/root/ashish/reservoir_lab/ENSIGN-42/ENSIGN_4.2/Ensign-CAPI/lib'\n",
    "\n",
    "os.putenv('LD_LIBRARY_PATH','/root/ashish/reservoir_lab/ENSIGN-42/ENSIGN_4.2/Ensign-CAPI/lib')\n",
    "\n",
    "#%env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ENSIGN Package for Tensor Decomposition\n",
    "import ensign.cp_decomp as cpd\n",
    "\n",
    "# Import ENSIGN Package to Convert csv Input Data to Tensor\n",
    "import ensign.csv2tensor as c2t\n",
    "\n",
    "# Import ENSIGN Package for Report Generation\n",
    "import ensign.report as report\n",
    "\n",
    "\n",
    "import ensign.sptensor as spt\n",
    "\n",
    "# Import ENSIGN Package for Visual Report Generation\n",
    "import ensign.visualize as viz\n",
    "\n",
    "#Import Anomaly Detection Modules\n",
    "from ensign.cyber_detectors.beaconing_detector import beacon_scores_from_decomp_mem, decomp_has_beacon_mem\n",
    "from ensign.cyber_detectors.network_mapping_detector import decomp_has_netmap_mem\n",
    "from ensign.cyber_detectors.portscan_detector import decomp_has_portscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get configuration\n",
    "with open('/root/ashish/reservoir_lab/ENSIGN-42/workflow_cfg.yml', 'r') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "    os.makedirs(cfg['save_dir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting CSV to tensor ...\n",
      "Setting up Dask client ...\n",
      "  Reading in /nvme_data14/CICDataset/CICDDoS2019/Dataset/csv/01-12-Backup/DrDoS_NetBIOS.csv ...\n",
      "  Reading in /nvme_data14/CICDataset/CICDDoS2019/Dataset/csv/01-12-Backup/DrDoS_MSSQL.csv ...\n",
      "  Reading in /nvme_data14/CICDataset/CICDDoS2019/Dataset/csv/01-12-Backup/DrDoS_LDAP.csv ...\n",
      "  Reading in /nvme_data14/CICDataset/CICDDoS2019/Dataset/csv/01-12-Backup/DrDoS_DNS.csv ...\n",
      "  Reading in /nvme_data15/CICDataset/DrDoS_NTP.csv ...\n",
      "  Reading in /nvme_data15/CICDataset/DrDoS_SNMP.csv ...\n",
      "  Reading in /nvme_data15/CICDataset/DrDoS_SSDP.csv ...\n",
      "  Reading in /nvme_data15/CICDataset/DrDoS_UDP.csv ...\n",
      "  Reading in /nvme_data15/CICDataset/Syn.csv ...\n",
      "  Reading in /nvme_data15/CICDataset/TFTP.csv ...\n",
      "  Reading in /nvme_data15/CICDataset/UDPLag.csv ...\n",
      "Validating ...\n",
      "Filtering ...\n",
      "Casting column types ...\n",
      "Binning ...\n",
      "  Binning mode 0 (second)\n",
      "  Binning mode 1 (none)\n",
      "  Binning mode 2 (none)\n",
      "  Binning mode 3 (none)\n",
      "  Binning mode 4 (log10)\n",
      "Fusing ...\n",
      "Calculating tensor entries ...\n",
      "Building sparse tensor ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://127.0.0.1:27095 remote=tcp://127.0.0.1:8786>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  csv2tensor took 246.41413187980652 seconds.\n",
      "Closing dask ...\n",
      "  Writing tensor files to disk ...\n",
      "  Writing ' Timestamp' to /root/ashish/reservoir_lab/ENSIGN-42/output/map_mode_0.txt (size=19031)\n",
      "  Writing ' Source IP' to /root/ashish/reservoir_lab/ENSIGN-42/output/map_mode_1.txt (size=550)\n",
      "  Writing ' Destination IP' to /root/ashish/reservoir_lab/ENSIGN-42/output/map_mode_2.txt (size=598)\n",
      "  Writing ' Destination Port' to /root/ashish/reservoir_lab/ENSIGN-42/output/map_mode_3.txt (size=65535)\n",
      "  Writing 'Flow Bytes/s' to /root/ashish/reservoir_lab/ENSIGN-42/output/map_mode_4.txt (size=10)\n",
      "  Writing 43730652 nonzeroes to /root/ashish/reservoir_lab/ENSIGN-42/output/tensor_data.txt\n",
      "    tensor dump took: 203.21294617652893 seconds\n"
     ]
    }
   ],
   "source": [
    "# Build tensor\n",
    "tensor = None\n",
    "if os.path.exists(os.path.join(cfg['save_dir'], 'tensor_data.txt')):\n",
    "    print('Tensor detected in save dir: {}, skipping csv2tensor.'.format(cfg['save_dir']))\n",
    "else:\n",
    "    print('Converting CSV to tensor ...')\n",
    "\n",
    "    print('Setting up Dask client ...')\n",
    "    start_dask = '{}/ENSIGN_4.2/Ensign-Py3/ensign/ensign_dask/start_dask.sh'.format(os.environ['ENSIGN_BASE'])\n",
    "    subprocess.run(['bash', start_dask, str(cfg['num_threads'])])\n",
    "    client = Client('127.0.0.1:8786')\n",
    "\n",
    "    dfs = []\n",
    "    for path in cfg['input_file'].split(' '):\n",
    "        for filename in glob(path):\n",
    "            print(f'  Reading in {filename} ...')\n",
    "            if cfg['bro_log']:\n",
    "                col_names = linecache.getline(os.path.join(cfg['save_dir'], filename), 7).split()[1:]\n",
    "                dfs.append(dd.read_csv(os.path.join(cfg['save_dir'], filename), sep='\\t', skiprows=8, header=0, names=col_names, usecols=cfg['columns']))\n",
    "            else:\n",
    "                dfs.append(dd.read_csv(os.path.join(cfg['save_dir'], filename), usecols=cfg['columns']))\n",
    "\n",
    "    start = time.time()\n",
    "    tensor = c2t.df2tensor(\n",
    "        dfs,\n",
    "        dask_client=client,\n",
    "        columns=cfg['columns'],\n",
    "        types=cfg['types'],\n",
    "        binning=cfg['binning']\n",
    "    )\n",
    "    end = time.time()\n",
    "    print('  csv2tensor took {} seconds.'.format(end - start))\n",
    "\n",
    "    print('Closing dask ...')\n",
    "    client.close()\n",
    "    stop_dask = '{}/ENSIGN_4.2/Ensign-Py3/ensign/ensign_dask/stop_dask.sh'.format(os.environ['ENSIGN_BASE'])\n",
    "    subprocess.check_output(['bash', stop_dask])\n",
    "\n",
    "    if cfg['dump_tensor_files']:\n",
    "        print('  Writing tensor files to disk ...')\n",
    "        start = time.time()\n",
    "        spt.write_sptensor(cfg['save_dir'], tensor)\n",
    "        end = time.time()\n",
    "        print(f'    tensor dump took: {end - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing tensor w/ CP APR ...\n",
      "  decomposition took 98.56361365318298 seconds.\n",
      "  Writing decomposition files to disk ...\n",
      "    decomp dump took: 1.9467532634735107 seconds\n"
     ]
    }
   ],
   "source": [
    "# CANDECOMP/PARAFAC (CP) decomposition \n",
    "if os.path.exists(os.path.join(cfg['save_dir'], 'weights.txt')):\n",
    "    print('weights.txt detected in save dir: {}, reading in decomposition ...'.format(cfg['save_dir']))\n",
    "    decomp = cpd.read_cp_decomp_dir(cfg['save_dir'])\n",
    "else:\n",
    "    print('Decomposing tensor w/ CP APR ...')\n",
    "    if cfg['num_threads'] < 1:\n",
    "        if 'OMP_NUM_THREADS' in os.environ.keys():\n",
    "            del os.environ['OMP_NUM_THREADS']\n",
    "    else:\n",
    "        os.environ['OMP_NUM_THREADS'] = str(cfg['num_threads'])\n",
    "\n",
    "    if tensor is None:\n",
    "        tensor = spt.read_sptensor(cfg['save_dir'])\n",
    "\n",
    "    start = time.time()\n",
    "    decomp = cpd.cp_apr(\n",
    "        tensor,\n",
    "        cfg['rank'],\n",
    "        max_outer_iter=cfg['max_outer_iter'],\n",
    "        max_inner_iter=cfg['max_inner_iter'],\n",
    "        mem_limit_gb=cfg['mem_limit_gb']\n",
    "    )\n",
    "    end = time.time()\n",
    "    print('  decomposition took {} seconds.'.format(end - start))\n",
    "\n",
    "    if cfg['dump_decomposition_files']:\n",
    "        print('  Writing decomposition files to disk ...')\n",
    "        start = time.time()\n",
    "        cpd.write_cp_decomp_dir(cfg['save_dir'], decomp, write_tensor=False)\n",
    "        end = time.time()\n",
    "        print(f'    decomp dump took: {end - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting visualization ...\n",
      "\n",
      "  visualization took 93.31784105300903 seconds.\n"
     ]
    }
   ],
   "source": [
    " # Generate Results Visualization\n",
    "if os.path.exists(os.path.join(cfg['save_dir'], 'viz.pdf')):\n",
    "    print('Visualization file detected in save dir: {}. Skipping visualization.'.format(cfg['save_dir']))\n",
    "else:\n",
    "    print('Starting visualization ...')\n",
    "    start = time.time()\n",
    "\n",
    "    with open(os.path.join(cfg['save_dir'], 'viz_beta_metadata.yml'), 'w') as f:\n",
    "        yaml.dump({\n",
    "            'time_mode': cfg['time_mode'],\n",
    "            'port_mode': cfg['port_mode'],\n",
    "            'use_detectors': cfg['use_detectors']\n",
    "        }, f)\n",
    "\n",
    "    viz.visualize(decomp, cfg['save_dir'])\n",
    "\n",
    "    convert_args = ['convert', '-delay', '10'] + ['{}_comp_{}.png'.format(i, i) for i in range(decomp.rank)] + [os.path.join(cfg['save_dir'], 'viz.pdf')]\n",
    "    print(subprocess.check_output(convert_args).decode('utf-8'))\n",
    "\n",
    "    for i in range(decomp.rank):\n",
    "        os.remove('{}_comp_{}.png'.format(i, i))\n",
    "\n",
    "    os.remove(os.path.join(cfg['save_dir'], 'viz_beta_metadata.yml'))\n",
    "\n",
    "    end = time.time()\n",
    "    print('  visualization took {} seconds.'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating textual report ...\n",
      "  report generation took 13.544945240020752 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Textual report generation\n",
    "if os.path.exists(os.path.join(cfg['save_dir'], 'report.txt')):\n",
    "    print('Report file detected in save dir: {}. Skipping report generation.'.format(cfg['save_dir']))\n",
    "else:\n",
    "    print('Generating textual report ...')\n",
    "    start = time.time()\n",
    "\n",
    "    domain_map_pattern = re.compile('domain_map_[0-9]+.tsv')\n",
    "    domain_map_fns = sorted(filter(lambda x: domain_map_pattern.fullmatch(x) is not None,\n",
    "                                       os.listdir(cfg['save_dir'])))\n",
    "    domain_maps = {\n",
    "        int(fn[11:fn.index('.tsv')]): report.read_domain_map(os.path.join(cfg['save_dir'], fn))\n",
    "            for fn in domain_map_fns\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(cfg['save_dir'], 'report.txt'), 'w') as f:\n",
    "        time_mode = None if cfg['time_mode'] == -1 else cfg['time_mode']\n",
    "        port_mode = None if cfg['port_mode'] == -1 else cfg['port_mode']\n",
    "        text_report = report.generate_report(decomp, domain_maps, time_mode, port_mode, cfg['use_detectors'])\n",
    "        f.write(text_report)\n",
    "\n",
    "    end = time.time()\n",
    "    print('  report generation took {} seconds.'.format(end - start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting detectors ...\n",
      "Running portscan detector ...\n",
      "Running network mapping detector ...\n",
      "Running beacon detector ...\n",
      "  detection took 1.4661805629730225 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Apply Detectors\n",
    "if cfg['use_detectors']:\n",
    "    if os.path.exists(os.path.join(cfg['save_dir'], 'detector_log')):\n",
    "        print('Detector log file detected in save dir: {}. Skipping detectors.'.format(cfg['save_dir']))\n",
    "    else:\n",
    "        print('Starting detectors ...')\n",
    "        start = time.time()\n",
    "        log = open(cfg['save_dir'] + '/detector_log', 'w')\n",
    "\n",
    "        print('Running portscan detector ...')\n",
    "        scans = decomp_has_portscan(decomp)\n",
    "        log.write(\"PORT SCAN RESULTS\\n\")\n",
    "        log.write(\"There are {} possible portscans\\n\\n\".format(len(scans)))\n",
    "        for comp, actor, machine in scans:\n",
    "            log.write(\"Component {} likely contains a port scan\\n\".format(comp))\n",
    "            log.write(\"Bad Actor: {}\\n\".format(actor))\n",
    "            log.write(\"Affected Machine: {}\\n\\n\".format(machine))\n",
    "\n",
    "        print('Running network mapping detector ...')\n",
    "        log.write(\"NETWORK MAPPING RESULTS\\n\")\n",
    "        netmaps = decomp_has_netmap_mem(decomp.factors, decomp.labels)\n",
    "        if len(netmaps) == 0:\n",
    "            log.write('No suspected netmaps\\n\\n')\n",
    "        for comp_id, bad_actor, attacked_port in netmaps:\n",
    "            log.write('Component {} likely contains a network mapping\\nBad actor: {}\\nAttacked Port: {}\\n\\n'.format(comp_id, bad_actor, attacked_port))\n",
    "\n",
    "        print('Running beacon detector ...')\n",
    "        log.write(\"BEACON RESULTS\\n\")\n",
    "        beacons = decomp_has_beacon_mem(decomp.factors)\n",
    "        log.write(\"The following components display beaconing behavior: {}\\n\\n\".format(beacons))\n",
    "\n",
    "        log.write(\"BEACON SCORES\\n\")\n",
    "        beacon_scores = beacon_scores_from_decomp_mem(decomp.factors)\n",
    "        log.write(\"Beacon scores for all components: {}\\n\".format(beacon_scores))\n",
    "\n",
    "        log.close()\n",
    "        end = time.time()\n",
    "        print('  detection took {} seconds.'.format(end - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
