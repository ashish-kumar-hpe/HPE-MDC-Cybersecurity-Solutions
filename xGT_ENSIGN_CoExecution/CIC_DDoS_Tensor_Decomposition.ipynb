{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os,os.path\n",
    "import linecache\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ.get('https_proxy'):\n",
    " del os.environ['https_proxy']\n",
    "if os.environ.get('http_proxy'):\n",
    " del os.environ['http_proxy']\n",
    "\n",
    "os.environ['ENSIGN_BASE']='/root/ashish/reservoir_lab/ENSIGN-42'\n",
    "os.environ['ANACONDA_HOME']='/root/anaconda3/'\n",
    "os.environ['PYTHONPATH']='/root/ashish/reservoir_lab/ENSIGN-42/ENSIGN_4.2/Ensign-Py3'\n",
    "\n",
    "sys.path.append('/root/ashish/reservoir_lab/ENSIGN-42/ENSIGN_4.2/bin')\n",
    "sys.path.append('/root/ashish/reservoir_lab/ENSIGN-42/ENSIGN_4.2/Ensign-Py3/bin')\n",
    "sys.path.append('/root/anaconda3/bin/')\n",
    "\n",
    "#os.environ['LD_LIBRARY_PATH']='/root/ashish/reservoir_lab/ENSIGN-42/ENSIGN_4.2/Ensign-CAPI/lib'\n",
    "\n",
    "os.putenv('LD_LIBRARY_PATH','/root/ashish/reservoir_lab/ENSIGN-42/ENSIGN_4.2/Ensign-CAPI/lib')\n",
    "\n",
    "#%env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ENSIGN Package for Tensor Decomposition\n",
    "import ensign.cp_decomp as cpd\n",
    "\n",
    "# Import ENSIGN Package to Convert csv Input Data to Tensor\n",
    "import ensign.csv2tensor as c2t\n",
    "\n",
    "# Import ENSIGN Package for Report Generation\n",
    "import ensign.report as report\n",
    "\n",
    "\n",
    "import ensign.sptensor as spt\n",
    "\n",
    "# Import ENSIGN Package for Visual Report Generation\n",
    "import ensign.visualize as viz\n",
    "\n",
    "#Import Anomaly Detection Modules\n",
    "from ensign.cyber_detectors.beaconing_detector import beacon_scores_from_decomp_mem, decomp_has_beacon_mem\n",
    "from ensign.cyber_detectors.network_mapping_detector import decomp_has_netmap_mem\n",
    "from ensign.cyber_detectors.portscan_detector import decomp_has_portscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get configuration\n",
    "with open('/root/ashish/reservoir_lab/ENSIGN-42/workflow_cfg.yml', 'r') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "    os.makedirs(cfg['save_dir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting CSV to tensor ...\n",
      "Setting up Dask client ...\n",
      "Ashish after setting up Dask Client ...\n",
      "Ashish inside else bro_log ...\n",
      "Validating ...\n",
      "Filtering ...\n",
      "Casting column types ...\n",
      "Binning ...\n",
      "  Binning mode 0 (second)\n",
      "  Binning mode 1 (none)\n",
      "  Binning mode 2 (none)\n",
      "  Binning mode 3 (none)\n",
      "  Binning mode 4 (log10)\n",
      "Fusing ...\n",
      "Calculating tensor entries ...\n",
      "Building sparse tensor ...\n",
      "Closing dask ...\n",
      "  Writing tensor files to disk ...\n",
      "  Writing ' Timestamp' to /root/ashish/reservoir_lab/ENSIGN-42/output/map_mode_0.txt (size=1862)\n",
      "  Writing ' Source IP' to /root/ashish/reservoir_lab/ENSIGN-42/output/map_mode_1.txt (size=149)\n",
      "  Writing ' Destination IP' to /root/ashish/reservoir_lab/ENSIGN-42/output/map_mode_2.txt (size=166)\n",
      "  Writing ' Destination Port' to /root/ashish/reservoir_lab/ENSIGN-42/output/map_mode_3.txt (size=65535)\n",
      "  Writing 'Flow Bytes/s' to /root/ashish/reservoir_lab/ENSIGN-42/output/map_mode_4.txt (size=10)\n",
      "  Writing 5938627 nonzeroes to /root/ashish/reservoir_lab/ENSIGN-42/output/tensor_data.txt\n",
      "  csv2tensor took 84.21011781692505 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Build tensor\n",
    "tensor = None\n",
    "if os.path.exists(os.path.join(cfg['save_dir'], 'tensor_data.txt')):\n",
    "    print('Tensor detected in save dir: {}, skipping csv2tensor.'.format(cfg['save_dir']))\n",
    "else:\n",
    "    print('Converting CSV to tensor ...')\n",
    "    start = time.time()\n",
    "\n",
    "    if cfg['dask_scheduler_address'] == -1:\n",
    "        client = None\n",
    "    else:\n",
    "        print('Setting up Dask client ...')\n",
    "        client = Client(cfg['dask_scheduler_address'])\n",
    "        print('Ashish after setting up Dask Client ...')\n",
    "\n",
    "    if cfg['bro_log']:\n",
    "        print('Ashish inside if bro_log ...')\n",
    "        col_names = linecache.getline(os.path.join(cfg['save_dir'], cfg['input_file']), 7).split()[1:]\n",
    "        df = dd.read_csv(os.path.join(cfg['save_dir'], cfg['input_file']), sep='\\t', skiprows=8, header=0, names=col_names, usecols=cfg['columns'])\n",
    "    else:\n",
    "        print('Ashish inside else bro_log ...')\n",
    "        df = dd.read_csv(os.path.join(cfg['save_dir'], cfg['input_file']), usecols=cfg['columns'])\n",
    "\n",
    "    tensor = c2t.df2tensor(\n",
    "        df,\n",
    "        dask_client=client,\n",
    "        columns=cfg['columns'],\n",
    "        types=cfg['types'],\n",
    "        binning=cfg['binning']\n",
    "    )\n",
    "\n",
    "    if cfg['dask_scheduler_address'] != -1:\n",
    "        print('Closing dask ...')\n",
    "        client.close()\n",
    "\n",
    "    if cfg['dump_tensor_files']:\n",
    "        print('  Writing tensor files to disk ...')\n",
    "        spt.write_sptensor(cfg['save_dir'], tensor)\n",
    "\n",
    "    end = time.time()\n",
    "    print('  csv2tensor took {} seconds.'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposing tensor w/ CP APR ...\n",
      "  Writing decomposition files to disk ...\n",
      "  decomposition took 84.7984368801117 seconds.\n"
     ]
    }
   ],
   "source": [
    "# CANDECOMP/PARAFAC (CP) decomposition \n",
    "if os.path.exists(os.path.join(cfg['save_dir'], 'weights.txt')):\n",
    "    print('weights.txt detected in save dir: {}, reading in decomposition ...'.format(cfg['save_dir']))\n",
    "    decomp = cpd.read_cp_decomp_dir(cfg['save_dir'])\n",
    "else:\n",
    "    print('Decomposing tensor w/ CP APR ...')\n",
    "    if cfg['omp_num_threads'] < 1:\n",
    "        if 'OMP_NUM_THREADS' in os.environ.keys():\n",
    "            del os.environ['OMP_NUM_THREADS']\n",
    "    else:\n",
    "        os.environ['OMP_NUM_THREADS'] = str(cfg['omp_num_threads'])\n",
    "\n",
    "    if tensor is None:\n",
    "        tensor = spt.read_sptensor(cfg['save_dir'])\n",
    "\n",
    "    start = time.time()\n",
    "    decomp = cpd.cp_apr(\n",
    "        tensor,\n",
    "        cfg['rank'],\n",
    "        max_outer_iter=cfg['max_outer_iter'],\n",
    "        max_inner_iter=cfg['max_inner_iter'],\n",
    "        mem_limit_gb=cfg['mem_limit_gb']\n",
    "    )\n",
    "    end = time.time()\n",
    "\n",
    "    if cfg['dump_decomposition_files']:\n",
    "        print('  Writing decomposition files to disk ...')\n",
    "        cpd.write_cp_decomp_dir(cfg['save_dir'], decomp, write_tensor=False)\n",
    "\n",
    "    print('  decomposition took {} seconds.'.format(end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting visualization ...\n",
      "\n",
      "  visualization took 42.603662967681885 seconds.\n"
     ]
    }
   ],
   "source": [
    " # Generate Results Visualization\n",
    "if os.path.exists(os.path.join(cfg['save_dir'], 'viz.pdf')):\n",
    "    print('Visualization file detected in save dir: {}. Skipping visualization.'.format(cfg['save_dir']))\n",
    "else:\n",
    "    print('Starting visualization ...')\n",
    "    start = time.time()\n",
    "\n",
    "    with open(os.path.join(cfg['save_dir'], 'viz_beta_metadata.yml'), 'w') as f:\n",
    "        yaml.dump({\n",
    "            'time_mode': cfg['time_mode'],\n",
    "            'port_mode': cfg['port_mode'],\n",
    "            'use_detectors': cfg['use_detectors']\n",
    "        }, f)\n",
    "\n",
    "    viz.visualize(decomp, cfg['save_dir'])\n",
    "\n",
    "    convert_args = ['convert', '-delay', '10'] + ['{}_comp_{}.png'.format(i, i) for i in range(decomp.rank)] + [os.path.join(cfg['save_dir'], 'viz.pdf')]\n",
    "    print(subprocess.check_output(convert_args).decode('utf-8'))\n",
    "\n",
    "    for i in range(decomp.rank):\n",
    "        os.remove('{}_comp_{}.png'.format(i, i))\n",
    "\n",
    "    os.remove(os.path.join(cfg['save_dir'], 'viz_beta_metadata.yml'))\n",
    "\n",
    "    end = time.time()\n",
    "    print('  visualization took {} seconds.'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating textual report ...\n",
      "  report generation took 1.7836129665374756 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Textual report generation\n",
    "if os.path.exists(os.path.join(cfg['save_dir'], 'report.txt')):\n",
    "    print('Report file detected in save dir: {}. Skipping report generation.'.format(cfg['save_dir']))\n",
    "else:\n",
    "    print('Generating textual report ...')\n",
    "    start = time.time()\n",
    "\n",
    "    domain_map_pattern = re.compile('domain_map_[0-9]+.tsv')\n",
    "    domain_map_fns = sorted(filter(lambda x: domain_map_pattern.fullmatch(x) is not None,\n",
    "                                       os.listdir(cfg['save_dir'])))\n",
    "    domain_maps = {\n",
    "        int(fn[11:fn.index('.tsv')]): report.read_domain_map(os.path.join(cfg['save_dir'], fn))\n",
    "            for fn in domain_map_fns\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(cfg['save_dir'], 'report.txt'), 'w') as f:\n",
    "        time_mode = None if cfg['time_mode'] == -1 else cfg['time_mode']\n",
    "        port_mode = None if cfg['port_mode'] == -1 else cfg['port_mode']\n",
    "        text_report = report.generate_report(decomp, domain_maps, time_mode, port_mode, cfg['use_detectors'])\n",
    "        f.write(text_report)\n",
    "\n",
    "    end = time.time()\n",
    "    print('  report generation took {} seconds.'.format(end - start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting detectors ...\n",
      "Running portscan detector ...\n",
      "Running network mapping detector ...\n",
      "Running beacon detector ...\n",
      "  detection took 0.7370562553405762 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Apply Detectors\n",
    "if cfg['use_detectors']:\n",
    "    if os.path.exists(os.path.join(cfg['save_dir'], 'detector_log')):\n",
    "        print('Detector log file detected in save dir: {}. Skipping detectors.'.format(cfg['save_dir']))\n",
    "    else:\n",
    "        print('Starting detectors ...')\n",
    "        start = time.time()\n",
    "        log = open(cfg['save_dir'] + '/detector_log', 'w')\n",
    "\n",
    "        print('Running portscan detector ...')\n",
    "        scans = decomp_has_portscan(decomp)\n",
    "        log.write(\"PORT SCAN RESULTS\\n\")\n",
    "        log.write(\"There are {} possible portscans\\n\\n\".format(len(scans)))\n",
    "        for comp, actor, machine in scans:\n",
    "            log.write(\"Component {} likely contains a port scan\\n\".format(comp))\n",
    "            log.write(\"Bad Actor: {}\\n\".format(actor))\n",
    "            log.write(\"Affected Machine: {}\\n\\n\".format(machine))\n",
    "\n",
    "        print('Running network mapping detector ...')\n",
    "        log.write(\"NETWORK MAPPING RESULTS\\n\")\n",
    "        netmaps = decomp_has_netmap_mem(decomp.factors, decomp.labels)\n",
    "        if len(netmaps) == 0:\n",
    "            log.write('No suspected netmaps\\n\\n')\n",
    "        for comp_id, bad_actor, attacked_port in netmaps:\n",
    "            log.write('Component {} likely contains a network mapping\\nBad actor: {}\\nAttacked Port: {}\\n\\n'.format(comp_id, bad_actor, attacked_port))\n",
    "\n",
    "        print('Running beacon detector ...')\n",
    "        log.write(\"BEACON RESULTS\\n\")\n",
    "        beacons = decomp_has_beacon_mem(decomp.factors)\n",
    "        log.write(\"The following components display beaconing behavior: {}\\n\\n\".format(beacons))\n",
    "\n",
    "        log.write(\"BEACON SCORES\\n\")\n",
    "        beacon_scores = beacon_scores_from_decomp_mem(decomp.factors)\n",
    "        log.write(\"Beacon scores for all components: {}\\n\".format(beacon_scores))\n",
    "\n",
    "        log.close()\n",
    "        end = time.time()\n",
    "        print('  detection took {} seconds.'.format(end - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
